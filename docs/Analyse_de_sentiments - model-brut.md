# Analyse de Sentiments grÃ¢ce au Deep Learning avec l'approche MLOps

> Cet article est disponible en ligne : [https://dev.to/davidscanu/analyse-de-sentiments-de-tweets-grace-au-deep-learning-une-approche-mlops-3ib7](https://dev.to/davidscanu/analyse-de-sentiments-de-tweets-grace-au-deep-learning-une-approche-mlops-3ib7)

![Les sentiments a travers les Tweet](images/Tweet.png)


*Cet article a Ã©tÃ© rÃ©digÃ© dans le cadre du projet : RÃ©alisez une analyse de sentiments grÃ¢ce au Deep Learning du parcours [AI Engineer](https://openclassrooms.com/fr/paths/795-ai-engineer). Les donnÃ©es utilisÃ©es sont issues du jeu de donnÃ©es open source [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140). Le code source complet est disponible sur [(https://github.com/Aragor85/mlflow_projectGitHub)]*

> ğŸ“ OpenClassrooms â€¢ Parcours [AI Engineer](https://openclassrooms.com/fr/paths/795-ai-engineer) | ğŸ‘‹ *Ã‰tudiant* : Djamel FERGUEN

![API: Analyse des sentiments a travers les Tweet](images/Tweet.png)


## ğŸŒ Contexte et problÃ©matique mÃ©tier 

Ce projet s'inscrit dans un scÃ©nario professionnel oÃ¹ j'interviens en tant qu'ingÃ©nieur IA chez MIC (Marketing Intelligence Consulting), entreprise de conseil spÃ©cialisÃ©e sur les problÃ©matiqus de marketing digital.

Notre client,  **Air Paradis** (compagnie aÃ©rienne), souhaite **anticiper les bad buzz sur les rÃ©seaux sociaux**. La mission consiste Ã  dÃ©velopper un produit IA permettant de prÃ©dire le sentiment associÃ© Ã  un tweet, afin d'amÃ©liorer son image de marque en ligne.

## âš¡ Mission

> DÃ©velopper un modÃ¨le d'IA permettant de prÃ©dire le sentiment associÃ© Ã  un tweet.

CrÃ©er un prototype fonctionnel d'un modÃ¨le d'analyse de sentiments pour tweets selon trois approches diffÃ©rentes :

1. **ModÃ¨le simple** : Approche classique (rÃ©gression logistique,Randomforest,LightGBM) pour une prÃ©diction rapide
2. **ModÃ¨le avancÃ©** : Utilisation de rÃ©seaux de neurones profonds avec diffÃ©rents word embeddings ( USE, Bidirectional_LSTM et BERT)
3. **ModÃ¨le avancÃ© BERT** : Le modÃ¨le BERT est bien intÃ©grÃ© dans le projet. Cependant, en raison de limitations matÃ©rielles (notamment l'absence de GPU et une configuration uniquement sur CPU), l'entraÃ®nement s'est avÃ©rÃ© extrÃªmement lent. Face Ã  un temps de calcul estimÃ© Ã  10 heures par Epoch, j'ai dÃ©cidÃ© d'interrompre l'exÃ©cution du modÃ¨le

Cette mission implique Ã©galement la mise en place d'une **dÃ©marche MLOps complÃ¨te pour le deploiment sur le Cloud** :

- Utilisation de **MLFlow pour le tracking des expÃ©rimentations et le stockage des modÃ¨les**.
- CrÃ©ation d'un **pipeline de dÃ©ploiement continu (Git + Github + plateforme Cloud Azure)**.
- IntÃ©gration de **tests unitaires automatisÃ©s**.
- Mise en place d'un **suivi de performance du modÃ©le en production** via Azure A[pplication Insight](https://learn.microsoft.com/fr-fr/azure/azure-monitor/app/app-insights-overview).

## ğŸ”§ Environnement technique

- **Distribution** : Anaconda ver. XX.XX
- **Langages** : Python ver. X.XX
- **BibliothÃ¨ques ML/DL** : Scikit-learn, TensorFlow/Keras, Transformers (BERT),  **Ajoute USE LSTM,......**
- **MLOps** : MLFlow, Git, GitHub Actions
- **Backend** : FastAPI
- **Frontend** : Next.js / React   
- **Monitoring** : Azure Application Insight
- **Traitement texte** : NLTK, Word Embeddings

## ğŸ›ï¸ Structure du projet

```
ğŸ“¦ mlflow_project/
â”£â”â” ğŸ“‚ app/
â”ƒ   â”£â”â” ğŸ“‚ model/                                   # Backend API de prÃ©diction
â”ƒ       â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ       â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ       â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ       â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ       â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights

â”£â”â” ğŸ“‚ .github/
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml          # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights

â”£â”â” ğŸ“‚ data/
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”£â”â” ğŸ“‚ docs/
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”£â”â” ğŸ“‚ images/
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”£â”â” ğŸ“‚ mlruns/
â”ƒ   â”£â”â” ğŸ“‚ 0/                                 # Backend API de prÃ©diction
â”ƒ       â”—â”â” ğŸ“‚ frontend/                                # Application Next.js
â”ƒ       ...
        â”—â”â” ğŸ“‚ frontend/                                # Application Next.js
â”ƒ       
â”£â”â” ğŸ“‚ models/
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
    ...  
â”ƒ   â”—â”â” ğŸ“ƒ analyse_sentiments_module-7.yml      # Guide de suivi des feedback utilisateur et des alertes avec Azure Application insights
â”—â”â” ğŸ“‚ notebooks/                           # Notebooks Jupyter pour l'analyse et modÃ¨les
    â”£â”â” ğŸ“ 01_Analyse_exploratoire.ipynb     # Exploration et visualisation des donnÃ©es
    
â”—â”â” ğŸ“ 04_Modele_BERT.ipynb              # DistilBERT pour analyse de sentiment
â”—â”â” ğŸ“ 04_Modele_BERT.ipynb              # DistilBERT pour analyse de sentiment
â”—â”â” ğŸ“ 04_Modele_BERT.ipynb              # DistilBERT pour analyse de sentiment
â”—â”â” ğŸ“ 04_Modele_BERT.ipynb              # DistilBERT pour analyse de sentiment
â”—â”â” ğŸ“ 04_Modele_BERT.ipynb              # DistilBERT pour analyse de sentiment

## ğŸ“” Notebooks du projet

- [ğŸ“Š Notebook 1 : Analyse exploratoire des donnÃ©es](https://github.com/DavidScanu/oc-ai-engineer-p07-analyse-sentiments-deep-learning/blob/main/notebooks/scanu-david-01-notebook-analyse-exploratoire-20250306.ipynb)

## ğŸ§­ Guides

- Help pour utilisation de l'API !!!! 

## ğŸ“‘ MÃ©thodologie et donnÃ©es

### Le jeu de donnÃ©es Sentiment140

Pour ce projet, nous avons utilisÃ© le jeu de donnÃ©es open source Sentiment140, qui contient 1,6 million de tweets annotÃ©s (nÃ©gative ou positive). Ce dataset comprend six champs principaux :

- **target** : la polaritÃ© du tweet (0 = nÃ©gatif, 1 = positif)
- **ids** : l'identifiant du tweet
- **date** : la date du tweet
- **flag** : une requÃªte Ã©ventuelle
- **user** : l'utilisateur ayant postÃ© le tweet
- **text** : le contenu textuel du tweet

J'ai choisi de rÃ©duire la taille du dataset a 16 000 tweets pour la suite du projet (configuration materiÃ©ls).

!!!!  reduction de la taille du dataset 

### Analyse exploratoire des donnÃ©es Sentiment140

Notre analyse exploratoire a rÃ©vÃ©lÃ© des caractÃ©ristiques distinctives importantes entre les tweets positifs et nÃ©gatifs :

- XX%  de tweets positifs
- XX%  de tweets nÃ©gatifs

Ã©quilibrÃ©s pas de smote 

### PrÃ©traitement des donnÃ©es textuelles

Un petit paragraphe pour dÃ©crire et surtout vÃ©rification ce que j'ai fait dans le premier Notebook  


Notre stratÃ©gie de prÃ©traitement s'est concentrÃ©e sur trois aspects clÃ©s :

1. **Traitement des Ã©lÃ©ments spÃ©ciaux** : PlutÃ´t que de simplement supprimer les URLs, mentions et hashtags, nous les avons remplacÃ©s par des tokens spÃ©ciaux (`<URL>`, `<MENTION>`) afin de prÃ©server l'information de leur prÃ©sence, tout en sÃ©parant les hashtags pour conserver leur contenu sÃ©mantique.

2. **Conservation des nÃ©gations** : Nous avons exclu les mots de nÃ©gation de la liste des stopwords pour prÃ©server le sens du sentiment exprimÃ©.

3. **Lemmatisation plutÃ´t que stemming** : AprÃ¨s avoir testÃ© les deux approches, nous avons privilÃ©giÃ© la lemmatisation qui prÃ©serve mieux le sens des mots tout en rÃ©duisant la dimensionnalitÃ© du vocabulaire.

## ğŸ§  Approches de modÃ©lisation

Pour rÃ©pondre Ã  la demande d'Air Paradis, nous avons dÃ©veloppÃ© et comparÃ© 5 approches de modÃ©lisation distinctes, de la plus simple Ã  la plus avancÃ©e.

### ModÃ¨le BERT (approche transformer)

### ModÃ¨le BERT (approche transformer)

### ModÃ¨le classique

Notre premiÃ¨re approche s'est basÃ©e sur des techniques classiques de machine learning, combinant une vectorisation du texte avec un classifieur traditionnel :

1. **Vectorisation** : transformation des textes en reprÃ©sentations numÃ©riques via TF-IDF (Term Frequency-Inverse Document Frequency)
2. **Classification** : utilisation d'une RÃ©gression Logistique pour prÃ©dire la polaritÃ© du sentiment

Cette approche prÃ©sente plusieurs avantages :
- RapiditÃ© d'entraÃ®nement et d'infÃ©rence
- Faible empreinte mÃ©moire
- Bonne interprÃ©tabilitÃ© des rÃ©sultats

MalgrÃ© sa simplicitÃ©, ce modÃ¨le a atteint une prÃ©cision (accuracy) de 79,8% sur notre jeu de test, ce qui constitue une base solide pour la dÃ©tection de sentiments nÃ©gatifs.

### ModÃ¨le sur mesure avancÃ© (rÃ©seaux de neurones avec word embeddings)

Pour notre deuxiÃ¨me approche, nous avons explorÃ© les techniques de deep learning avec des embeddings de mots et des rÃ©seaux de neurones rÃ©currents. Nous avons d'abord optimisÃ© notre environnement pour utiliser efficacement le GPU disponible (GTX 1060 3GB) :

1. **Optimisations matÃ©rielles** :
   - DÃ©sactivation du recurrent_dropout pour permettre l'utilisation de CuDNNLSTM optimisÃ©
   - Activation de XLA (Accelerated Linear Algebra) pour optimiser les graphes d'opÃ©rations
   - Utilisation de la prÃ©cision mixte (float16/float32)
   - Augmentation de la taille du batch Ã  256 pour exploiter le parallÃ©lisme
   - Optimisation du pipeline de donnÃ©es avec tf.data.Dataset et prefetch

2. **Word Embeddings** : nous avons comparÃ© deux techniques d'embeddings pour reprÃ©senter les mots dans un espace vectoriel dense :
   - Word2Vec prÃ©-entraÃ®nÃ© sur un large corpus de tweets
   - GloVe (Global Vectors for Word Representation)

3. **Architecture du rÃ©seau** : nous avons implÃ©mentÃ© un rÃ©seau de neurones bidirectionnel avec plusieurs couches LSTM et des mÃ©canismes de rÃ©gularisation :

```python
def create_optimized_lstm_model(embedding_matrix, max_seq_length=MAX_SEQUENCE_LENGTH, trainable=False):
    vocab_size, embedding_dim = embedding_matrix.shape
    
    # EntrÃ©e du modÃ¨le
    input_layer = tf.keras.layers.Input(shape=(max_seq_length,))
    
    # Couche d'embedding avec des poids prÃ©-entraÃ®nÃ©s
    embedding_layer = tf.keras.layers.Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        weights=[embedding_matrix],
        input_length=max_seq_length,
        trainable=trainable
    )(input_layer)
    
    # Dropout spatial
    dropout_1 = tf.keras.layers.SpatialDropout1D(0.3)(embedding_layer)
    
    # Couche LSTM bidirectionnelle optimisÃ©e pour GPU
    lstm_layer = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(
            units=128,
            dropout=0.2,
            recurrent_dropout=0.0,  # Optimisation GPU
            return_sequences=True
        )
    )(dropout_1)
    
    # DeuxiÃ¨me couche LSTM
    lstm_layer_2 = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(
            units=64,
            dropout=0.2,
            recurrent_dropout=0.0  # Optimisation GPU
        )
    )(lstm_layer)
    
    # Couche dense avec activation ReLU
    dense_1 = tf.keras.layers.Dense(64, activation='relu')(lstm_layer_2)
    dropout_2 = tf.keras.layers.Dropout(0.4)(dense_1)
    
    # Couche de sortie
    output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(dropout_2)
    
    # CrÃ©er et compiler le modÃ¨le
    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)
    
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    
    return model
```

L'architecture de notre modÃ¨le LSTM comprend :
- Une couche d'embedding initialisÃ©e avec les poids prÃ©-entraÃ®nÃ©s Word2Vec
- Un dropout spatial pour rÃ©duire la corrÃ©lation entre les features consÃ©cutives
- Deux couches LSTM bidirectionnelles (128 puis 64 unitÃ©s) pour capturer les dÃ©pendances contextuelles
- Des couches de dropout pour la rÃ©gularisation et Ã©viter le surapprentissage
- Une couche dense intermÃ©diaire avec activation ReLU 
- Une couche de sortie avec activation sigmoÃ¯de pour la classification binaire

Les rÃ©sultats de l'entraÃ®nement montrent une progression constante de l'accuracy, comme on peut le voir sur les graphiques ci-dessous :

![Courbe d'apprentissage](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/48j9lz9bh84os9nkp2bz.png)

Cette approche plus sophistiquÃ©e nous a permis d'atteindre une prÃ©cision de 81,8% sur l'ensemble de validation, avec un score de 85,2% sur le jeu d'entraÃ®nement, surpassant ainsi le modÃ¨le simple.

### ModÃ¨le BERT (approche transformer)

Pour notre troisiÃ¨me approche, nous avons explorÃ© l'Ã©tat de l'art en NLP en utilisant BERT (Bidirectional Encoder Representations from Transformers) :

1. **ModÃ¨le prÃ©-entraÃ®nÃ©** : nous avons utilisÃ© DistilBERT, une version allÃ©gÃ©e et distillÃ©e de BERT, pour rÃ©duire les coÃ»ts de calcul tout en maintenant des performances Ã©levÃ©es
2. **Fine-tuning** : nous avons affinÃ© le modÃ¨le sur notre jeu de donnÃ©es spÃ©cifique d'analyse de sentiments

Pour cette approche, nous avons utilisÃ© le modÃ¨le `DistilBertForSequenceClassification` de la bibliothÃ¨que Hugging Face, qui est spÃ©cifiquement conÃ§u pour les tÃ¢ches de classification de sÃ©quences textuelles :

```python
def train_bert_sentiment(data_path, model_name="distilbert-base-uncased", batch_size=4, epochs=3, sample_size=20000):
    """
    Fonction principale pour l'entraÃ®nement du modÃ¨le DistilBERT sur une tÃ¢che d'analyse de sentiments.
    """

    # DÃ©finir les paramÃ¨tres
    params = {
        'model_name': model_name,
        'batch_size': batch_size,
        'learning_rate': 2e-5,
        'epochs': epochs,
        'max_length': 128,
        'sample_size': sample_size
    }

    # Charger les donnÃ©es
    print("Chargement du dataset...")
    column_names = ['target', 'ids', 'date', 'flag', 'user', 'text']
    raw_data = pd.read_csv(data_path, encoding='utf-8', names=column_names)

    # PrÃ©parer les donnÃ©es
    print("PrÃ©paration des donnÃ©es...")
    data_splits = prepare_data(raw_data, sample_size=sample_size)

    # Initialiser le tokenizer et le modÃ¨le
    print("Initialisation du modÃ¨le DistilBERT...")
    tokenizer = DistilBertTokenizer.from_pretrained(model_name)
    model = DistilBertForSequenceClassification.from_pretrained(
        model_name,
        num_labels=2  # Sentiment binaire (0 = nÃ©gatif, 1 = positif)
    )

    # Ajustement du batch size selon la mÃ©moire GPU
    adjusted_batch_size = min(8, batch_size)
    
    # CrÃ©ation des datasets et des dataloaders
    train_dataset = TweetDataset(data_splits['train']['texts'], data_splits['train']['labels'], tokenizer)
    val_dataset = TweetDataset(data_splits['val']['texts'], data_splits['val']['labels'], tokenizer)
    test_dataset = TweetDataset(data_splits['test']['texts'], data_splits['test']['labels'], tokenizer)

    train_loader = DataLoader(train_dataset, batch_size=adjusted_batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=adjusted_batch_size * 2)
    test_loader = DataLoader(test_dataset, batch_size=adjusted_batch_size * 2)

    # DÃ©tection du device (GPU/CPU)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    # EntraÃ®nement du modÃ¨le avec accumulation de gradients pour optimiser l'utilisation mÃ©moire
    gradient_accumulation_steps = max(1, 16 // adjusted_batch_size)
    history, metrics = train_model(
        model,
        train_loader,
        val_loader,
        test_loader,
        device,
        epochs=epochs,
        gradient_accumulation_steps=gradient_accumulation_steps
    )

    # Enregistrement du modÃ¨le dans MLflow
    run_id = log_model_to_mlflow(model, tokenizer, model_name, metrics, params)

    return {
        'model': model,
        'tokenizer': tokenizer,
        'metrics': metrics,
        'run_id': run_id
    }
```

DistilBERT est particuliÃ¨rement adaptÃ© Ã  notre tÃ¢che car il :
- Utilise une **architecture transformer bidirectionnelle** pour capturer le contexte dans les deux directions
- A Ã©tÃ© **prÃ©-entraÃ®nÃ© sur un large corpus de textes**, ce qui lui permet de comprendre les nuances linguistiques
- Est **40% plus lÃ©ger que BERT** tout en conservant 97% de ses performances
- S'intÃ¨gre parfaitement dans un **pipeline MLOps** grÃ¢ce Ã  son API standardisÃ©e

Cette approche **transformers** nous a permis d'atteindre une **prÃ©cision de 91,3 %**, dÃ©montrant la puissance des **architectures basÃ©es sur l'attention** pour la comprÃ©hension du langage naturel.

### Comparaison des performances des modÃ¨les

Voici un rÃ©capitulatif des performances obtenues avec nos diffÃ©rentes approches :

| ModÃ¨le | PrÃ©cision (Accuracy) | F1-Score | Temps d'entraÃ®nement | Taille du modÃ¨le |
|--------|----------------------|----------|---------------------|-----------------|
| RÃ©gression Logistique + TF-IDF | 79,8% | 0,797 | ~5 minutes | ~15 MB |
| LSTM + Word2Vec | 85,2% | 0,851 | ~2 heures | ~90 MB |
| LSTM + GloVe | 84,7% | 0,846 | ~2 heures | ~88 MB |
| DistilBERT fine-tunÃ© | 91,3% | 0,912 | ~4 heures (GPU) | ~250 MB |

Pour le dÃ©ploiement en production, nous avons retenu le modÃ¨le **LSTM avec Word2Vec**, qui offre le meilleur compromis entre performance et ressources requises. Bien que DistilBERT ait obtenu de meilleurs rÃ©sultats, sa taille et ses exigences en termes de ressources de calcul le rendaient moins adaptÃ© Ã  un dÃ©ploiement sur une infrastructure Cloud gratuite.

## âš™ï¸ Mise en Å“uvre du MLOps

### Principes du MLOps

**Le MLOps (Machine Learning Operations) est une mÃ©thodologie qui vise Ã  standardiser et Ã  automatiser le cycle de vie des modÃ¨les de machine learning**, de leur dÃ©veloppement Ã  leur dÃ©ploiement en production. Pour ce projet, nous avons mis en Å“uvre plusieurs principes clÃ©s du MLOps :

1. **ReproductibilitÃ©** : environnement de dÃ©veloppement versionnÃ© et documentÃ©
2. **Automatisation** : pipeline de dÃ©ploiement continu
3. **Monitoring** : suivi des performances du modÃ¨le en production
4. **AmÃ©lioration continue** : collecte de feedback et rÃ©entraÃ®nement pÃ©riodique

Cette approche nous a permis de crÃ©er une solution robuste et Ã©volutive pour Air Paradis.

### Tracking des expÃ©rimentations avec MLFlow

Pour assurer une gestion efficace des expÃ©rimentations, nous avons utilisÃ© [MLFlow](https://mlflow.org/docs/latest/index.html), un outil open-source spÃ©cialisÃ© dans le **suivi et la gestion des modÃ¨les de machine learning** :

1. **Tracking des mÃ©triques** : pour chaque expÃ©rimentation, nous avons enregistrÃ© automatiquement les paramÃ¨tres du modÃ¨le, les mÃ©triques de performance (accuracy, F1-score, prÃ©cision, rappel) et les artefacts gÃ©nÃ©rÃ©s
2. **Centralisation des modÃ¨les** : tous les modÃ¨les entraÃ®nÃ©s ont Ã©tÃ© stockÃ©s de maniÃ¨re centralisÃ©e avec leurs mÃ©tadonnÃ©es
3. **Visualisation** : l'interface utilisateur de MLFlow nous a permis de comparer visuellement les diffÃ©rentes expÃ©rimentations

![Serveur MLFLow](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/btvbhmrrjepcj6cj6zw7.png)

Cette approche nous a permis de garder une trace claire de l'Ã©volution de nos modÃ¨les et de sÃ©lectionner objectivement le plus performant pour le dÃ©ploiement.

## ğŸ’» Interface utilisateur

### Architecture de l'application

Notre solution se compose de deux parties principales :

1. **Backend (FastAPI)** :
   - API REST exposant le modÃ¨le d'analyse de sentiments
   - Endpoints pour la prÃ©diction individuelle et par lots
   - SystÃ¨me de feedback et de monitoring
   - TÃ©lÃ©chargement automatique des artefacts du modÃ¨le depuis MLFlow

![Page /docs du serveur FastAPI](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/telklwg14wd6h1q3jwk1.png)

2. **Frontend (Next.js)** :
   - Interface utilisateur intuitive et responsive
   - Mode clair/sombre pour le confort visuel
   - Visualisation des rÃ©sultats de prÃ©diction
   - SystÃ¨me de collecte de feedback
   - Widget d'indication de connexion avec l'API

![Mockup macbook de l'application frontend](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/p9kesxlbg2d6tjod1xce.png)

### FonctionnalitÃ©s de l'interface utilisateur

L'interface utilisateur offre plusieurs fonctionnalitÃ©s clÃ©s pour faciliter l'analyse de sentiments :

1. **Analyse individuelle** : prÃ©diction du sentiment d'un tweet unique
2. **Exemples prÃ©dÃ©finis** : tweets d'exemple positifs et nÃ©gatifs
3. **Historique** : conservation des analyses prÃ©cÃ©dentes
4. **Feedback** : possibilitÃ© de signaler des prÃ©dictions incorrectes

Cette interface a Ã©tÃ© conÃ§ue pour Ãªtre **intuitive et accessible aux Ã©quipes marketing d'Air Paradis**, sans nÃ©cessiter de connaissances techniques approfondies.

## ğŸ”„ Pipeline de dÃ©ploiement continu

Pour automatiser le dÃ©ploiement de notre modÃ¨le, nous avons mis en place un **pipeline CI/CD (IntÃ©gration Continue / DÃ©ploiement Continu)** avec les composants suivants :

1. **Versionnement du code** : utilisation de Git pour le contrÃ´le de version
2. **GitHub Actions** : automatisation des tests et du dÃ©ploiement Ã  chaque push sur la branche principale
3. **DÃ©ploiement sur Heroku** : plateforme Cloud pour hÃ©berger notre API de prÃ©diction

### Tests unitaires automatisÃ©s

Pour garantir la fiabilitÃ© de notre solution, nous avons implÃ©mentÃ© des **tests unitaires automatisÃ©s** couvrant les aspects critiques :

1. **Test du endpoint de santÃ©** : VÃ©rifie que l'API rÃ©pond correctement sur `/health` avec un code 200 et confirme que le statut retournÃ© est "ok". Le modÃ¨le est chargÃ© correctement.
2. **Test du endpoint de prÃ©diction** : S'assure que l'API traite correctement les requÃªtes POST sur `/predict`, accepte un texte Ã  analyser et renvoie un rÃ©sultat contenant les champs "sentiment" et "confidence".

```python
def test_health_endpoint(client):
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "ok"

def test_predict_endpoint(client):
    response = client.post("/predict", json={"text": "I love flying with this airline!"})
    assert response.status_code == 200
    result = response.json()
    assert "sentiment" in result
    assert "confidence" in result
```

### GitHub Actions 

Le dÃ©ploiement est entiÃ¨rement automatisÃ© grÃ¢ce Ã  **GitHub Actions** :

1. **DÃ©clenchement** : Ã€ chaque commit/push sur la branche principale, GitHub Actions lance le workflow.
2. **Tests automatisÃ©s** : Le workflow exÃ©cute tous les tests unitaires.
3. **DÃ©ploiement conditionnel** : Uniquement si les tests rÃ©ussissent, l'application est dÃ©ployÃ©e automatiquement sur Heroku.

#### CrÃ©ation du workflow GitHub Actions

Pour la crÃ©ation du workflow GitHub Actions, nous crÃ©ons un fichier `.github/workflows/heroku-deploy.yml` Ã  la racine dont voici le contenu :

```yaml
name: Deploy to Heroku

on:
  push:
    branches:
      - main
    paths:
      - 'app/fastapi/**'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      - name: Install dependencies
        working-directory: ./app/fastapi
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Run tests
        working-directory: ./app/fastapi
        run: |
          python -m pytest tests/test_api.py -v
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          RUN_ID: ${{ secrets.RUN_ID }}
          APPINSIGHTS_INSTRUMENTATION_KEY: ${{ secrets.APPINSIGHTS_INSTRUMENTATION_KEY }}

  deploy:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Install Heroku CLI
        run: |
            curl https://cli-assets.heroku.com/install.sh | sh
      - name: Deploy to Heroku
        uses: akhileshns/heroku-deploy@v3.12.12
        with:
          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}
          heroku_app_name: "air-paradis-sentiment-api"
          heroku_email: ${{ secrets.HEROKU_EMAIL }}
          appdir: "app/fastapi"
          region: "eu"
```

#### Configuration des secrets GitHub

Le workflow **GitHub Actions** a besoin d'accÃ©der aux **variables d'environnement**. Nous avons donc renseigner les "secrets" nÃ©cessaires. Dans notre dÃ©pÃ´t GitHub, nous allons dans "Settings" > "Secrets and variables" > "Actions", puis nous cliquons sur "New repository secret". Nous ajoutons les secrets suivants:

| Nom du secret | Description |
|---------------|-------------|
| `HEROKU_API_KEY` | ClÃ© API Heroku |
| `HEROKU_EMAIL` | Email du compte Heroku |
| `MLFLOW_TRACKING_URI` | URI du serveur MLflow |
| `RUN_ID` | ID du run MLflow |
| `APPINSIGHTS_INSTRUMENTATION_KEY` | ClÃ© Application Insights |

### DÃ©ploiement sur Heroku

Pour le dÃ©ploiement de notre solution, nous avons choisi [Heroku](https://www.heroku.com/) pour plusieurs raisons :

1. **Plan gratuit** : conforme Ã  la demande de limiter les coÃ»ts pour ce prototype
2. **IntÃ©gration avec GitHub** : facilite le dÃ©ploiement continu avec GitHub Actions
3. **ScalabilitÃ©** : possibilitÃ© d'Ã©voluer si le projet est approuvÃ© pour la production
4. **RÃ©gion Europe** : conformitÃ© avec les exigences de localisation des donnÃ©es

#### Configuration Heroku

Notre application utilise les fichiers de configuration suivants pour Heroku :

- **Procfile** : `web: uvicorn main:app --host=0.0.0.0 --port=${PORT:-8000}`
- **runtime.txt** : `python-3.10.12`
- **requirements.txt** : Liste de toutes les dÃ©pendances nÃ©cessaires

Les variables d'environnement sur Heroku incluent :
- `MLFLOW_TRACKING_URI` : URI du serveur MLflow
- `RUN_ID` : Identifiant du run MLflow du modÃ¨le dÃ©ployÃ©
- `APPINSIGHTS_INSTRUMENTATION_KEY` : ClÃ© pour Azure Application Insights

### Exemple d'exÃ©cution et dÃ©ploiement rÃ©ussis

La capture d'Ã©cran suivante indique les **tests ont Ã©tÃ© passÃ©s avec succÃ¨s** et que le dÃ©ploiement est rÃ©ussi sur **Heroku**.

![Capture d'Ã©cran d'un run GitHub Actions](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/f8kea1d9cidmc6bp272r.png)

### Avantages de notre pipeline CI/CD

Notre pipeline de dÃ©ploiement continu offre plusieurs avantages significatifs :

1. **Automatisation complÃ¨te** : Aucune intervention manuelle nÃ©cessaire
2. **FiabilitÃ© accrue** : Tests systÃ©matiques rÃ©duisant les risques
3. **TraÃ§abilitÃ©** : Chaque dÃ©ploiement liÃ© Ã  un commit Git spÃ©cifique
4. **Feedback rapide** : Information immÃ©diate en cas de problÃ¨me

Cette approche MLOps moderne nous permet de nous concentrer sur l'amÃ©lioration de notre modÃ¨le d'analyse de sentiment plutÃ´t que sur les aspects opÃ©rationnels, tout en garantissant que chaque nouvelle version est correctement validÃ©e avant la mise en production.

## ğŸ“¡ Suivi de la performance en production

### Mise en place d'Azure Application Insights

Pour assurer un suivi efficace des performances du modÃ¨le en production, nous avons intÃ©grÃ© Azure Application Insights, un service d'analyse des performances applicatives :

1. **TÃ©lÃ©mÃ©trie** : collecte automatique des donnÃ©es de performance de l'API
2. **Ã‰vÃ©nements personnalisÃ©s** : enregistrement d'Ã©vÃ©nements spÃ©cifiques liÃ©s au modÃ¨le
3. **Visualisation** : tableaux de bord pour suivre l'Ã©volution des performances

Cette intÃ©gration nous permet de disposer d'une vision complÃ¨te du comportement de notre modÃ¨le en situation rÃ©elle.

### SystÃ¨me de feedback utilisateur

Un Ã©lÃ©ment clÃ© de notre approche MLOps est la collecte de feedback utilisateur sur les prÃ©dictions du modÃ¨le :

1. **Interface de validation** : pour chaque prÃ©diction, l'utilisateur peut indiquer si elle est correcte ou non
2. **Collecte structurÃ©e** : enregistrement du tweet, de la prÃ©diction initiale et de la correction Ã©ventuelle
3. **Stockage centralisÃ©** : toutes les donnÃ©es de feedback sont centralisÃ©es dans Azure Application Insights

Dans Azure Application Insights, pour consulter les **feedbacks de tweets incorrectement prÃ©dits**, il suffit d'exÃ©cuter la commande suivante : 

```kusto
customEvents
| where name == "model_feedback" and customDimensions.is_correct == "False"
| sort by timestamp desc
| project timestamp, 
        tweet = tostring(customDimensions.tweet), 
        prediction = tostring(customDimensions.prediction), 
        corrected_sentiment = tostring(customDimensions.corrected_sentiment)
```

![Feedbacks de tweets incorrectement prÃ©dits ](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/59roizfuhudinm7fjfol.png)

Ce systÃ¨me permet de **constituer progressivement un corpus d'exemples difficiles qui serviront Ã  amÃ©liorer le modÃ¨le**. Ces exemples difficiles sont particuliÃ¨rement prÃ©cieux car ils reprÃ©sentent les cas limites oÃ¹ le modÃ¨le actuel Ã©choue, rÃ©vÃ©lant ainsi ses points faibles spÃ©cifiques.

En collectant systÃ©matiquement ces tweets mal classifiÃ©s, nous crÃ©ons un **dataset enrichi qui cible prÃ©cisÃ©ment les lacunes du modÃ¨le**. Cette approche d'apprentissage actif (*active learning*) est beaucoup plus efficace qu'une simple augmentation de donnÃ©es alÃ©atoire, car elle concentre les efforts d'amÃ©lioration sur les zones problÃ©matiques.

### Configuration des alertes automatiques

Pour dÃ©tecter rapidement les problÃ¨mes potentiels, nous avons configurÃ© un **systÃ¨me d'alertes automatiques** :

1. **DÃ©finition du seuil** : dÃ©clenchement d'une alerte si **3 prÃ©dictions incorrectes sont signalÃ©es dans un intervalle de 5 minutes**.
2. **Notification** : envoi d'un email aux responsables du projet.
3. **Suivi** : journalisation des alertes pour analyse ultÃ©rieure.

![Capture de l'Ã©cran alertes de Azure Application Insights](https://dev-to-uploads.s3.amazonaws.com/uploads/articles/siytp14vvvye8ray3ax1.png)

Ce mÃ©canisme proactif permet Ã  l'Ã©quipe d'intervenir rapidement en cas de dÃ©gradation des performances du modÃ¨le.

### StratÃ©gie d'amÃ©lioration continue du modÃ¨le

Pour garantir la pertinence du modÃ¨le dans le temps, nous dÃ©finissons une stratÃ©gie d'amÃ©lioration continue :

1. **Analyse pÃ©riodique** : examen mensuel des tweets mal prÃ©dits pour identifier des patterns
2. **Enrichissement des donnÃ©es** : ajout des exemples difficiles au jeu d'entraÃ®nement
3. **RÃ©entraÃ®nement** : mise Ã  jour trimestrielle du modÃ¨le avec les nouvelles donnÃ©es
4. **DÃ©ploiement automatisÃ©** : mise en production de la nouvelle version via le pipeline CI/CD

Cette approche cyclique permet d'adapter le modÃ¨le Ã  l'Ã©volution du langage sur Twitter et aux spÃ©cificitÃ©s des conversations concernant Air Paradis.

## ğŸ Conclusion

### RÃ©sultats obtenus

Ce projet nous a permis de dÃ©velopper un **prototype fonctionnel d'analyse de sentiments pour tweets**, rÃ©pondant pleinement aux attentes d'Air Paradis :

1. **Performance** : notre modÃ¨le **LSTM avec Word2Vec** atteint une prÃ©cision de 85,2%, offrant une dÃ©tection fiable des sentiments nÃ©gatifs.
2. **DÃ©ploiement** : la solution est accessible via une API REST dÃ©ployÃ©e sur Heroku.
3. **Interface** : une application ergonomique permet aux Ã©quipes marketing d'utiliser facilement le modÃ¨le.
4. **Monitoring** : un systÃ¨me complet de suivi et d'alertes garantit la dÃ©tection rapide des problÃ¨mes potentiels.

### Perspectives d'Ã©volution

Si ce prototype est validÃ© par Air Paradis, plusieurs axes d'amÃ©lioration pourraient Ãªtre explorÃ©s :

1. **DÃ©ploiement du modÃ¨le BERT** : migration vers une infrastructure permettant d'exploiter les performances supÃ©rieures de BERT
2. **Analyse en temps rÃ©el** : intÃ©gration avec l'API Twitter pour une surveillance continue
3. **Classification multi-classes** : distinction entre sentiments nÃ©gatifs, neutres et positifs
4. **Analyse thÃ©matique** : identification des sujets spÃ©cifiques gÃ©nÃ©rant des sentiments nÃ©gatifs

### Avantages pour Air Paradis

Cette solution d'analyse de sentiments offre plusieurs avantages stratÃ©giques pour Air Paradis :

1. **DÃ©tection prÃ©coce** : identification des bad buzz potentiels avant qu'ils ne prennent de l'ampleur
2. **RÃ©activitÃ©** : capacitÃ© Ã  intervenir rapidement sur les problÃ¨mes signalÃ©s
3. **Intelligence client** : meilleure comprÃ©hension des prÃ©occupations et des attentes des clients
4. **Protection de l'image** : prÃ©servation de la rÃ©putation de la compagnie sur les rÃ©seaux sociaux

En conclusion, ce projet illustre comment **les technologies d'IA, combinÃ©es Ã  une approche MLOps structurÃ©e, peuvent apporter une rÃ©elle valeur ajoutÃ©e dans la gestion de la rÃ©putation en ligne d'une entreprise**. Air Paradis dispose dÃ©sormais d'un outil puissant pour anticiper et gÃ©rer efficacement sa prÃ©sence sur les rÃ©seaux sociaux.