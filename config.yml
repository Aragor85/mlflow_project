grid_search:
  logistic_regression:
    C: [0.01, 0.1, 1.0, 10.0]
    max_iter: [100, 500, 1000]

  random_forest:
    n_estimators: [100, 200, 300]
    max_depth: [10, 20, 30]
    random_state: [42]

  lightgbm:
    n_estimators: [100, 200, 300]
    learning_rate: [0.05, 0.1]
    num_leaves: [31, 40, 50]
    random_state: [42]

use:
  model_url: "https://tfhub.dev/google/universal-sentence-encoder/4"
  classifier_type: "mlp"  # ou "logistic_regression"
  mlp:
    dense_units: 128
    dropout: 0.3
    epochs: 5
    batch_size: 32
    learning_rate: 0.0001


lstm:
  max_num_words: 20000
  max_sequence_length: 128
  embedding_dim: 128
  lstm_units: 64
  dropout: 0.2
  batch_size: 32
  epochs: 5


bert:
  model_name: "bert-base-uncased"
  max_sequence_length: 128
  batch_size: 32
  epochs: 1
  learning_rate: 0.005
