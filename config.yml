grid_search:
  logistic_regression:
    C: [0.01, 0.1, 1.0, 10.0]
    max_iter: [100, 500]

  random_forest:
    n_estimators: [100, 200]
    max_depth: [10, 20, 30]
    random_state: [42]

  lightgbm:
    n_estimators: [100, 200]
    learning_rate: [0.05, 0.1]
    num_leaves: [30, 40]
    random_state: [42]

use:
  model_url: "https://tfhub.dev/google/universal-sentence-encoder/4"
  classifier_type: "mlp"
  mlp_params:                 
    hidden_layer_sizes: [256, 128, 64]
    activation: relu
    solver: adam
    alpha: 0.0001
    learning_rate_init: 0.001
    early_stopping: true
    max_iter: 200
    random_state: 42

lstm:
  max_num_words: 20000
  max_sequence_length: 128
  embedding_dim: 128
  lstm_units: 64
  dropout: 0.4
  batch_size: 32
  epochs: 7

bert:
  model_name: "distilbert-base-uncased"
  max_sequence_length: 128
  batch_size: 32
  epochs: 4
  learning_rate: 0.005
